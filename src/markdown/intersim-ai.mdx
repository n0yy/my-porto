# ğŸŒŸ From Data to Deployment: My Role as a Machine Learning Engineer in InterSim Project

**ğŸš€ Tags:** BERT, IntersimQS, LLaMA-3.2, HuggingFace, FastAPI, Capstone Project

---

### ğŸ§ **Overview**

InterSim was not just a capstone projectâ€”it was a **mission**. ğŸ¯ The goal? To empower job seekers, especially fresh graduates, by equipping them with the tools to ace job interviews. ğŸ’¼ As a **Machine Learning Engineer**, I contributed to transforming an ambitious idea into a functional **AI-driven platform** that simplifies interview preparation. Here's how I made it happen:

---

### ğŸ“Š **Step 1: Data Collection and Preprocessing**

![Scrapping Data Job in Glints using Automa](/scrap.PNG)

Everything starts with **data**. ğŸ—‚ï¸ My first task was to scrape job descriptions and resumes using **Automa**. This process involved navigating structured and unstructured data sources to create a robust dataset. Once collected:

- ğŸ§¹ I cleaned and preprocessed the data.
- ğŸ” Ensured that it was ready for embedding and model training.

This foundational step laid the groundwork for the platform's functionality. ğŸ—ï¸

---

### ğŸ§  **Step 2: Embedding Data and Calculating Similarity**

![Calc Similarity](/calc_simil.png)

To provide a **personalized interview experience**, I implemented a custom **JobSimilarityModel** using **BERT** (Bidirectional Encoder Representations from Transformers). Here's how it worked:

- **ğŸ”— Tokenization and Embedding Generation:**
  Using BERT's tokenizer and pre-trained model, I processed job descriptions into embeddings that capture their semantic meaning. This step leveraged **batch processing** to optimize performance. ğŸï¸

- **ğŸ“ Similarity Calculation:**
  The **cosine similarity** metric was applied to measure the relevance between a user's profile and various job descriptions. For example, the model could identify the **top 10 job roles** most similar to a given user profile by comparing embeddings. ğŸ¯

The implementation combined the **precision of BERT** with scalable batch processing to produce meaningful similarity scores. This feature powered InterSim's **question-generation system**, providing tailored interview questions based on job descriptions. ğŸ¤–ğŸ’¬

---

### ğŸ”§ **Step 3: Fine-Tuning the Model**

![Tuning model](/tuning.PNG)

The heart of InterSim lies in its **AI model**. ğŸ«€ Using **LLaMA-3.2-8B**, I fine-tuned the model with a dataset formatted in **Alpaca style**. Here's what I achieved:

- ğŸ”„ **Trained the model for 60 epochs** with rigorous optimization.
- ğŸ“‰ Achieved a **training loss of 0.22**, a testament to its accuracy and effectiveness. âœ…

This fine-tuned model became the brain behind InterSim's personalized interview question generator. ğŸ§ âœ¨

---

### ğŸ–§ **Step 4: Building Scalable Infrastructure**

![Model Uploaded](/hf.PNG)

Creating a powerful AI model is just the beginning; making it accessible to users is the real challenge. ğŸ—ï¸ Here's what I did:

- â˜ï¸ Uploaded the fine-tuned model to **HuggingFace**.
- ğŸŒ Designed a scalable **RESTful API** using **FastAPI** to serve as the bridge between our backend services and the user-facing mobile app. ğŸ“±

This ensured that the model was **always available, reliable, and ready to serve users** anytime, anywhere. ğŸŒâš¡

---

### ğŸ› ï¸ **Technologies Behind the Scenes**

Throughout the project, I worked with cutting-edge tools and technologies, including:

- ğŸ§‘â€ğŸ’» **Machine Learning Tools:** PyTorch, Unsloth, HuggingFace, BERT, LLaMA.
- ğŸ”§ **Development Frameworks:** FastAPI, Docker.

---

### ğŸŒ **Here's Our Work**

ğŸ“‚ [GitHub Repository](https://github.com/InterSim-lab)

Feel free to explore our code and learn more about the project! ğŸ‘©â€ğŸ’»ğŸ‘¨â€ğŸ’»
